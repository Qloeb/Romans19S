{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\"\n",
    "textes = []\n",
    "labels = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    path_file = os.path.join(path, file)\n",
    "\n",
    "    if os.path.isfile(path_file):\n",
    "        # Détecter l'encodage du fichier\n",
    "        with open(path_file, 'rb') as f:\n",
    "            encoding = chardet.detect(f.read())['encoding']\n",
    "\n",
    "        # Ouvrir le fichier avec l'encodage détecté\n",
    "        with open(path_file, encoding=encoding) as fp:\n",
    "            soup = BeautifulSoup(fp, \"html.parser\")\n",
    "\n",
    "        paragraphes = soup.select(\"p\")\n",
    "        for p in paragraphes:\n",
    "            texte = p.get_text()\n",
    "            textes.append(texte)\n",
    "            labels.append(file.split(\"-\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rassembler_textes_et_labels(textes, labels, taille_minimale=1000):\n",
    "    textes_rassembles = []\n",
    "    labels_rassembles = []\n",
    "\n",
    "    buffer_texte = \"\"\n",
    "    buffer_label = \"\"\n",
    "\n",
    "    for texte, label in zip(textes, labels):\n",
    "        if buffer_label == \"\":\n",
    "            buffer_label = label\n",
    "\n",
    "        if buffer_label == label:\n",
    "            buffer_texte += \" \" + texte\n",
    "            if len(buffer_texte) >= taille_minimale:\n",
    "                textes_rassembles.append(buffer_texte)\n",
    "                labels_rassembles.append(buffer_label)\n",
    "                buffer_texte = \"\"\n",
    "                buffer_label = \"\"\n",
    "        else:\n",
    "            if len(buffer_texte) >= taille_minimale:\n",
    "                textes_rassembles.append(buffer_texte)\n",
    "                labels_rassembles.append(buffer_label)\n",
    "            buffer_texte = texte\n",
    "            buffer_label = label\n",
    "\n",
    "    # Ajoute le dernier échantillon s'il n'a pas été ajouté précédemment et s'il est assez long\n",
    "    if buffer_label and len(buffer_texte) >= taille_minimale:\n",
    "        textes_rassembles.append(buffer_texte)\n",
    "        labels_rassembles.append(buffer_label)\n",
    "\n",
    "    return textes_rassembles, labels_rassembles\n",
    "\n",
    "textes_rassembles, labels_rassembles = rassembler_textes_et_labels(textes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'échantillons rassemblés\n",
      "Balzac : 1667\n",
      "Flaubert : 1887\n",
      "Maupassant : 966\n",
      "Sand : 1922\n",
      "Zola : 3826\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre d'échantillons rassemblés\")\n",
    "print(\"Balzac :\",labels_rassembles.count(\"balzac\"))\n",
    "print(\"Flaubert :\",labels_rassembles.count(\"flaubert\"))\n",
    "print(\"Maupassant :\",labels_rassembles.count(\"maupassant\"))\n",
    "print(\"Sand :\",labels_rassembles.count(\"sand\"))\n",
    "print(\"Zola :\",labels_rassembles.count(\"zola\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70037 mots uniques trouvés.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Tokenisation\n",
    "max_len = 500  # Longueur maximale des séquences\n",
    "max_words = 10000  # Nombre maximum de mots à considérer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(textes_rassembles)\n",
    "sequences = tokenizer.texts_to_sequences(textes_rassembles)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"{len(word_index)} mots uniques trouvés.\")\n",
    "\n",
    "# Padding\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Encodage des labels\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels_rassembles)\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels_categorical, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction et entraînement du modèle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,412,229\n",
      "Trainable params: 1,412,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "206/206 [==============================] - 674s 3s/step - loss: 1.3449 - accuracy: 0.4474 - val_loss: 1.0366 - val_accuracy: 0.5393\n",
      "Epoch 2/20\n",
      "206/206 [==============================] - 662s 3s/step - loss: 0.8458 - accuracy: 0.6302 - val_loss: 1.1326 - val_accuracy: 0.4680\n",
      "Epoch 3/20\n",
      "206/206 [==============================] - 673s 3s/step - loss: 0.7134 - accuracy: 0.6979 - val_loss: 0.6291 - val_accuracy: 0.7578\n",
      "Epoch 4/20\n",
      "206/206 [==============================] - 646s 3s/step - loss: 0.4835 - accuracy: 0.8174 - val_loss: 0.5418 - val_accuracy: 0.8113\n",
      "Epoch 5/20\n",
      "206/206 [==============================] - 603s 3s/step - loss: 0.2848 - accuracy: 0.9107 - val_loss: 0.3899 - val_accuracy: 0.8667\n",
      "Epoch 6/20\n",
      "206/206 [==============================] - 598s 3s/step - loss: 0.2123 - accuracy: 0.9359 - val_loss: 0.3500 - val_accuracy: 0.8831\n",
      "Epoch 7/20\n",
      "206/206 [==============================] - 588s 3s/step - loss: 0.0834 - accuracy: 0.9773 - val_loss: 0.3017 - val_accuracy: 0.9044\n",
      "Epoch 8/20\n",
      "206/206 [==============================] - 580s 3s/step - loss: 0.0407 - accuracy: 0.9897 - val_loss: 0.3041 - val_accuracy: 0.9178\n",
      "Epoch 9/20\n",
      "206/206 [==============================] - 554s 3s/step - loss: 0.0331 - accuracy: 0.9903 - val_loss: 0.2822 - val_accuracy: 0.9227\n",
      "Epoch 10/20\n",
      "206/206 [==============================] - 553s 3s/step - loss: 0.0356 - accuracy: 0.9904 - val_loss: 0.2756 - val_accuracy: 0.9306\n",
      "Epoch 11/20\n",
      "206/206 [==============================] - 552s 3s/step - loss: 0.0681 - accuracy: 0.9807 - val_loss: 0.4136 - val_accuracy: 0.8868\n",
      "Epoch 12/20\n",
      "206/206 [==============================] - 555s 3s/step - loss: 0.0251 - accuracy: 0.9916 - val_loss: 0.3233 - val_accuracy: 0.9081\n",
      "Epoch 13/20\n",
      "206/206 [==============================] - 550s 3s/step - loss: 0.0798 - accuracy: 0.9755 - val_loss: 0.4028 - val_accuracy: 0.8831\n",
      "Epoch 14/20\n",
      "206/206 [==============================] - 553s 3s/step - loss: 0.0436 - accuracy: 0.9863 - val_loss: 0.4666 - val_accuracy: 0.8551\n",
      "Epoch 15/20\n",
      "206/206 [==============================] - 549s 3s/step - loss: 0.0357 - accuracy: 0.9897 - val_loss: 0.2912 - val_accuracy: 0.9245\n",
      "Epoch 16/20\n",
      "206/206 [==============================] - 562s 3s/step - loss: 0.0426 - accuracy: 0.9874 - val_loss: 0.6486 - val_accuracy: 0.8344\n",
      "Epoch 17/20\n",
      "206/206 [==============================] - 556s 3s/step - loss: 0.0483 - accuracy: 0.9851 - val_loss: 0.3795 - val_accuracy: 0.8862\n",
      "Epoch 18/20\n",
      "206/206 [==============================] - 551s 3s/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.4153 - val_accuracy: 0.8904\n",
      "Epoch 19/20\n",
      "206/206 [==============================] - 558s 3s/step - loss: 0.0509 - accuracy: 0.9831 - val_loss: 0.3867 - val_accuracy: 0.9069\n",
      "Epoch 20/20\n",
      "206/206 [==============================] - 551s 3s/step - loss: 0.0155 - accuracy: 0.9932 - val_loss: 0.3753 - val_accuracy: 0.9160\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 128, input_length=max_len),\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 23s 344ms/step - loss: 0.4669 - accuracy: 0.8944\n",
      "Accuracy: 89.44%\n",
      "65/65 [==============================] - 24s 369ms/step - loss: 0.4669 - accuracy: 0.8944\n",
      "Accuracy: 89.44%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode(text, label):\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"tf\"), label\n",
    "\n",
    "encoded_data = [encode(text, label) for text, label in zip(textes_rassembles, labels_rassembles)]\n",
    "\n",
    "input_ids = np.array([data[0][\"input_ids\"].numpy() for data in encoded_data])\n",
    "attention_mask = np.array([data[0][\"attention_mask\"].numpy() for data in encoded_data])\n",
    "\n",
    "le = LabelEncoder()\n",
    "numeric_labels = le.fit_transform(labels_rassembles)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_ids, numeric_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_mask, input_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8214, 512)\n",
      "train_masks shape: (8214, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.squeeze(X_train)\n",
    "train_masks = np.squeeze(train_masks)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"train_masks shape:\", train_masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 536M/536M [00:14<00:00, 37.6MB/s] \n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(np.unique(labels_rassembles))\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,512,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResourceGather]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m train_data \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mbatch(\u001b[39m1\u001b[39m)  \u001b[39m# Use a smaller batch size, e.g. 2\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Train the model with gradient accumulation\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m train_with_gradient_accumulation(model, train_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, steps_per_update\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_with_gradient_accumulation\u001b[39m(model, train_dataset, val_dataset, epochs, steps_per_update, optimizer):\n\u001b[0;32m     41\u001b[0m     loss_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36mtrain_with_gradient_accumulation\u001b[1;34m(model, dataset, epochs, steps_per_update, optimizer)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m dataset:\n\u001b[0;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> 16\u001b[0m         logits \u001b[39m=\u001b[39m model(inputs, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     17\u001b[0m         loss_value \u001b[39m=\u001b[39m loss_fn(targets, logits)\n\u001b[0;32m     18\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss_value, model\u001b[39m.\u001b[39mtrainable_weights)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1057\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1056\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1057\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1060\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    419\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:1665\u001b[0m, in \u001b[0;36mTFBertForSequenceClassification.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[0;32m   1636\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mbatch_size, sequence_length\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1637\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1658\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m   1659\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[39m    labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m \u001b[39m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \u001b[39m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m \u001b[39m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1664\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1665\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1666\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   1667\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1668\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1669\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1670\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1671\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1672\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1673\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1674\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1675\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m   1676\u001b[0m     )\n\u001b[0;32m   1677\u001b[0m     pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1678\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(inputs\u001b[39m=\u001b[39mpooled_output, training\u001b[39m=\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1057\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1056\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1057\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1060\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    419\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:791\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     token_type_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfill(dims\u001b[39m=\u001b[39minput_shape, value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 791\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m    792\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    793\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    794\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    795\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    796\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m    797\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m    798\u001b[0m )\n\u001b[0;32m    800\u001b[0m \u001b[39m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[39m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m attention_mask_shape \u001b[39m=\u001b[39m shape_list(attention_mask)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1057\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1056\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1057\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1060\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:213\u001b[0m, in \u001b[0;36mTFBertEmbeddings.call\u001b[1;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39m# Note: tf.gather, on which the embedding layer is based, won't check positive out of bound\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[39m# indices on GPU, returning zeros instead. This is a dangerous silent behavior.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     tf\u001b[39m.\u001b[39mdebugging\u001b[39m.\u001b[39massert_less(\n\u001b[0;32m    206\u001b[0m         input_ids,\n\u001b[0;32m    207\u001b[0m         tf\u001b[39m.\u001b[39mcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size, dtype\u001b[39m=\u001b[39minput_ids\u001b[39m.\u001b[39mdtype),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m         ),\n\u001b[0;32m    212\u001b[0m     )\n\u001b[1;32m--> 213\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mgather(params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, indices\u001b[39m=\u001b[39;49minput_ids)\n\u001b[0;32m    215\u001b[0m input_shape \u001b[39m=\u001b[39m shape_list(inputs_embeds)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    217\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5069\u001b[0m, in \u001b[0;36mgather_v2\u001b[1;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[0;32m   5061\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgather\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   5062\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   5063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather_v2\u001b[39m(params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5067\u001b[0m               batch_dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m   5068\u001b[0m               name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 5069\u001b[0m   \u001b[39mreturn\u001b[39;00m gather(\n\u001b[0;32m   5070\u001b[0m       params,\n\u001b[0;32m   5071\u001b[0m       indices,\n\u001b[0;32m   5072\u001b[0m       validate_indices\u001b[39m=\u001b[39;49mvalidate_indices,\n\u001b[0;32m   5073\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   5074\u001b[0m       axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5075\u001b[0m       batch_dims\u001b[39m=\u001b[39;49mbatch_dims)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:549\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    542\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    543\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    544\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[0;32m    548\u001b[0m           instructions)\n\u001b[1;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5056\u001b[0m, in \u001b[0;36mgather\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   5051\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39mgather_v2(\n\u001b[0;32m   5052\u001b[0m       params, indices, axis, batch_dims\u001b[39m=\u001b[39mbatch_dims, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   5053\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   5054\u001b[0m   \u001b[39m# TODO(apassos) find a less bad way of detecting resource variables\u001b[39;00m\n\u001b[0;32m   5055\u001b[0m   \u001b[39m# without introducing a circular dependency.\u001b[39;00m\n\u001b[1;32m-> 5056\u001b[0m   \u001b[39mreturn\u001b[39;00m params\u001b[39m.\u001b[39;49msparse_read(indices, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   5057\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m   5058\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39mgather_v2(params, indices, axis, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:713\u001b[0m, in \u001b[0;36mBaseResourceVariable.sparse_read\u001b[1;34m(self, indices, name)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mGather\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m name) \u001b[39mas\u001b[39;00m name:\n\u001b[0;32m    712\u001b[0m   variable_accessed(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 713\u001b[0m   value \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mresource_gather(\n\u001b[0;32m    714\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, indices, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    716\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mvariant:\n\u001b[0;32m    717\u001b[0m     \u001b[39m# For DT_VARIANT types, the handle's shape_and_type[1:] stores the\u001b[39;00m\n\u001b[0;32m    718\u001b[0m     \u001b[39m# variant's handle data.  Extract it.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m     handle_data \u001b[39m=\u001b[39m get_eager_safe_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:548\u001b[0m, in \u001b[0;36mresource_gather\u001b[1;34m(resource, indices, dtype, batch_dims, validate_indices, name)\u001b[0m\n\u001b[0;32m    546\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    547\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 548\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m    549\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m    550\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thuym\\anaconda3\\envs\\proj-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6941\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6939\u001b[0m message \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6940\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 6941\u001b[0m six\u001b[39m.\u001b[39;49mraise_from(core\u001b[39m.\u001b[39;49m_status_to_exception(e\u001b[39m.\u001b[39;49mcode, message), \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,512,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ResourceGather]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Custom training loop\n",
    "def train_with_gradient_accumulation(model, dataset, epochs, steps_per_update, optimizer):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        step = 0\n",
    "        total_loss = 0.0\n",
    "        total_steps = 0\n",
    "\n",
    "        for inputs, targets in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(inputs, training=True)\n",
    "                loss_value = loss_fn(targets, logits)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            if step % steps_per_update == 0:\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "                grads = [tf.zeros_like(w) for w in model.trainable_weights]\n",
    "            else:\n",
    "                grads = [g + w for g, w in zip(grads, model.trainable_weights)]\n",
    "            step += 1\n",
    "            total_loss += loss_value\n",
    "            total_steps += 1\n",
    "\n",
    "            metric.update_state(targets, logits)\n",
    "\n",
    "        print(f\"Loss: {total_loss / total_steps}, Accuracy: {metric.result().numpy()}\")\n",
    "        metric.reset_states()\n",
    "\n",
    "# Convert your data to a TensorFlow dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_train, \"attention_mask\": train_masks}, y_train))\n",
    "train_data = train_data.batch(1)  # Use a smaller batch size, e.g. 2\n",
    "\n",
    "# Train the model with gradient accumulation\n",
    "train_with_gradient_accumulation(model, train_data, epochs=20, steps_per_update=4, optimizer=optimizer)\n",
    "\n",
    "def train_with_gradient_accumulation(model, train_dataset, val_dataset, epochs, steps_per_update, optimizer):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        step = 0\n",
    "        total_loss = 0.0\n",
    "        total_steps = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, targets in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(inputs, training=True)\n",
    "                loss_value = loss_fn(targets, logits)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            if step % steps_per_update == 0:\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "                grads = [tf.zeros_like(w) for w in model.trainable_weights]\n",
    "            else:\n",
    "                grads = [g + w for g, w in zip(grads, model.trainable_weights)]\n",
    "            step += 1\n",
    "            total_loss += loss_value\n",
    "            total_steps += 1\n",
    "\n",
    "            train_metric.update_state(targets, logits)\n",
    "\n",
    "        # Validation loop\n",
    "        for val_inputs, val_targets in val_dataset:\n",
    "            val_logits = model(val_inputs, training=False)\n",
    "            val_metric.update_state(val_targets, val_logits)\n",
    "\n",
    "        print(f\"Train Loss: {total_loss / total_steps}, Train Accuracy: {train_metric.result().numpy()}, Validation Accuracy: {val_metric.result().numpy()}\")\n",
    "        train_metric.reset_states()\n",
    "        val_metric.reset_states()\n",
    "\n",
    "# Convert your data to TensorFlow datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_train, \"attention_mask\": train_masks}, y_train)).batch(2)\n",
    "val_data = tf.data.Dataset.from_tensor_slices(({\"input_ids\": X_test, \"attention_mask\": test_masks}, y_test)).batch(2)\n",
    "\n",
    "# Train the model with gradient accumulation and validation\n",
    "train_with_gradient_accumulation(model, train_data, val_data, epochs=20, steps_per_update=4, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = model.evaluate([X_test, test_masks], y_test, batch_size=8)\n",
    "print(f\"Accuracy: {eval_results[1] * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
