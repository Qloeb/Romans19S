{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\"\n",
    "textes = []\n",
    "labels = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    path_file = os.path.join(path, file)\n",
    "\n",
    "    if os.path.isfile(path_file):\n",
    "        # Détecter l'encodage du fichier\n",
    "        with open(path_file, 'rb') as f:\n",
    "            encoding = chardet.detect(f.read())['encoding']\n",
    "\n",
    "        # Ouvrir le fichier avec l'encodage détecté\n",
    "        with open(path_file, encoding=encoding) as fp:\n",
    "            soup = BeautifulSoup(fp, \"html.parser\")\n",
    "\n",
    "        paragraphes = soup.select(\"p\")\n",
    "        for p in paragraphes:\n",
    "            texte = p.get_text()\n",
    "            textes.append(texte)\n",
    "            labels.append(file.split(\"-\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rassembler_textes_et_labels(textes, labels, taille_minimale=1000):\n",
    "    textes_rassembles = []\n",
    "    labels_rassembles = []\n",
    "\n",
    "    buffer_texte = \"\"\n",
    "    buffer_label = \"\"\n",
    "\n",
    "    for texte, label in zip(textes, labels):\n",
    "        if buffer_label == \"\":\n",
    "            buffer_label = label\n",
    "\n",
    "        if buffer_label == label:\n",
    "            buffer_texte += \" \" + texte\n",
    "            if len(buffer_texte) >= taille_minimale:\n",
    "                textes_rassembles.append(buffer_texte)\n",
    "                labels_rassembles.append(buffer_label)\n",
    "                buffer_texte = \"\"\n",
    "                buffer_label = \"\"\n",
    "        else:\n",
    "            if len(buffer_texte) >= taille_minimale:\n",
    "                textes_rassembles.append(buffer_texte)\n",
    "                labels_rassembles.append(buffer_label)\n",
    "            buffer_texte = texte\n",
    "            buffer_label = label\n",
    "\n",
    "    # Ajoute le dernier échantillon s'il n'a pas été ajouté précédemment et s'il est assez long\n",
    "    if buffer_label and len(buffer_texte) >= taille_minimale:\n",
    "        textes_rassembles.append(buffer_texte)\n",
    "        labels_rassembles.append(buffer_label)\n",
    "\n",
    "    return textes_rassembles, labels_rassembles\n",
    "\n",
    "textes_rassembles, labels_rassembles = rassembler_textes_et_labels(textes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'échantillons rassemblés\n",
      "Balzac : 1667\n",
      "Flaubert : 1887\n",
      "Maupassant : 966\n",
      "Sand : 1922\n",
      "Zola : 3826\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre d'échantillons rassemblés\")\n",
    "print(\"Balzac :\",labels_rassembles.count(\"balzac\"))\n",
    "print(\"Flaubert :\",labels_rassembles.count(\"flaubert\"))\n",
    "print(\"Maupassant :\",labels_rassembles.count(\"maupassant\"))\n",
    "print(\"Sand :\",labels_rassembles.count(\"sand\"))\n",
    "print(\"Zola :\",labels_rassembles.count(\"zola\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70037 mots uniques trouvés.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Tokenisation\n",
    "max_len = 500  # Longueur maximale des séquences\n",
    "max_words = 10000  # Nombre maximum de mots à considérer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(textes_rassembles)\n",
    "sequences = tokenizer.texts_to_sequences(textes_rassembles)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"{len(word_index)} mots uniques trouvés.\")\n",
    "\n",
    "# Padding\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Encodage des labels\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels_rassembles)\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction et entraînement du modèle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70036 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Paramètres\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Prétraitement des données\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(textes_rassembles)\n",
    "sequences = tokenizer.texts_to_sequences(textes_rassembles)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Found {len(word_index)} unique tokens.\")\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels_rassembles)\n",
    "encoded_labels = encoder.transform(labels_rassembles)\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, categorical_labels, test_size=0.2, random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle LSTM\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(len(encoder.classes_), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle avec cross-validation...\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 13s 93ms/step - loss: 1.3731 - accuracy: 0.4237\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 9s 92ms/step - loss: 0.8031 - accuracy: 0.6629\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 10s 92ms/step - loss: 0.5274 - accuracy: 0.7891\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 10s 95ms/step - loss: 0.2900 - accuracy: 0.9037\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 10s 98ms/step - loss: 0.2011 - accuracy: 0.9361\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 9s 90ms/step - loss: 0.2992 - accuracy: 0.9095\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 9s 90ms/step - loss: 0.2518 - accuracy: 0.9231\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 9s 89ms/step - loss: 0.1482 - accuracy: 0.9600\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 9s 91ms/step - loss: 0.0556 - accuracy: 0.9852\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 9s 87ms/step - loss: 0.0636 - accuracy: 0.9814\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 9s 89ms/step - loss: 0.0255 - accuracy: 0.9930\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 9s 89ms/step - loss: 0.0340 - accuracy: 0.9886\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 10s 93ms/step - loss: 0.0204 - accuracy: 0.9930\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 10s 93ms/step - loss: 0.0595 - accuracy: 0.9816\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 9s 88ms/step - loss: 0.0299 - accuracy: 0.9904\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 10s 94ms/step - loss: 0.0555 - accuracy: 0.9862\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 11s 106ms/step - loss: 0.0305 - accuracy: 0.9915\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.0264 - accuracy: 0.9913\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.3054 - accuracy: 0.8958\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 13s 122ms/step - loss: 0.1796 - accuracy: 0.9411\n",
      "52/52 [==============================] - 3s 40ms/step - loss: 0.5205 - accuracy: 0.8509\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 14s 104ms/step - loss: 1.2962 - accuracy: 0.4526\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.7858 - accuracy: 0.6445\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 1.0531 - accuracy: 0.6002\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 11s 106ms/step - loss: 0.7942 - accuracy: 0.7433\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 9s 90ms/step - loss: 0.4374 - accuracy: 0.8445\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 0.2430 - accuracy: 0.9130\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.1437 - accuracy: 0.9548\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 0.0797 - accuracy: 0.9770\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 11s 110ms/step - loss: 0.0642 - accuracy: 0.9822\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 11s 107ms/step - loss: 0.0280 - accuracy: 0.9932\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.0389 - accuracy: 0.9909\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 10s 93ms/step - loss: 0.0226 - accuracy: 0.9938\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 9s 88ms/step - loss: 0.0153 - accuracy: 0.9948\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 9s 87ms/step - loss: 0.0392 - accuracy: 0.9877\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.0247 - accuracy: 0.9925\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 11s 107ms/step - loss: 0.0141 - accuracy: 0.9954\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.0174 - accuracy: 0.9941\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 0.0146 - accuracy: 0.9942\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 11s 105ms/step - loss: 0.0122 - accuracy: 0.9947\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 12s 120ms/step - loss: 0.0126 - accuracy: 0.9945\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.3117 - accuracy: 0.9282\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 13s 106ms/step - loss: 1.3305 - accuracy: 0.4471\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 11s 106ms/step - loss: 0.9732 - accuracy: 0.5792\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.5775 - accuracy: 0.7664\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 9s 88ms/step - loss: 0.3967 - accuracy: 0.8437\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 10s 94ms/step - loss: 0.2808 - accuracy: 0.9122\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 9s 87ms/step - loss: 0.1431 - accuracy: 0.9591\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 9s 87ms/step - loss: 0.1060 - accuracy: 0.9722\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 9s 88ms/step - loss: 0.0481 - accuracy: 0.9886\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 10s 94ms/step - loss: 0.0376 - accuracy: 0.9903\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 10s 96ms/step - loss: 0.0202 - accuracy: 0.9936\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 10s 97ms/step - loss: 0.8260 - accuracy: 0.7926\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 9s 90ms/step - loss: 0.8771 - accuracy: 0.6841\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 10s 98ms/step - loss: 0.5711 - accuracy: 0.8130\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 11s 104ms/step - loss: 0.3873 - accuracy: 0.8889\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 12s 113ms/step - loss: 0.3451 - accuracy: 0.9003\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 12s 117ms/step - loss: 0.2176 - accuracy: 0.9375\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 12s 115ms/step - loss: 0.1395 - accuracy: 0.9609\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 12s 120ms/step - loss: 0.0758 - accuracy: 0.9836\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 12s 118ms/step - loss: 0.0516 - accuracy: 0.9868\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 11s 107ms/step - loss: 0.0344 - accuracy: 0.9912\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 0.5989 - accuracy: 0.8527\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 12s 94ms/step - loss: 1.3641 - accuracy: 0.4299\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.7626 - accuracy: 0.7077\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 13s 125ms/step - loss: 0.6496 - accuracy: 0.7614\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 13s 125ms/step - loss: 0.2898 - accuracy: 0.8910\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 12s 112ms/step - loss: 0.5518 - accuracy: 0.8303\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 15s 138ms/step - loss: 0.2591 - accuracy: 0.9247\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 12s 115ms/step - loss: 0.1477 - accuracy: 0.9607\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 11s 106ms/step - loss: 0.0676 - accuracy: 0.9843\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.0692 - accuracy: 0.9808\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 13s 124ms/step - loss: 0.0414 - accuracy: 0.9890\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.0379 - accuracy: 0.9903\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 11s 108ms/step - loss: 0.0480 - accuracy: 0.9863\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 11s 109ms/step - loss: 0.0225 - accuracy: 0.9922\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 13s 130ms/step - loss: 0.0275 - accuracy: 0.9912\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 12s 117ms/step - loss: 0.0199 - accuracy: 0.9930\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 10s 102ms/step - loss: 0.0190 - accuracy: 0.9927\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 0.0407 - accuracy: 0.9871\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 9s 91ms/step - loss: 0.0507 - accuracy: 0.9878\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 9s 89ms/step - loss: 0.0809 - accuracy: 0.9753\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 9s 90ms/step - loss: 0.0735 - accuracy: 0.9747\n",
      "52/52 [==============================] - 2s 31ms/step - loss: 0.5718 - accuracy: 0.8484\n",
      "Epoch 1/20\n",
      "103/103 [==============================] - 12s 89ms/step - loss: 1.3387 - accuracy: 0.4361\n",
      "Epoch 2/20\n",
      "103/103 [==============================] - 9s 91ms/step - loss: 0.6869 - accuracy: 0.7103\n",
      "Epoch 3/20\n",
      "103/103 [==============================] - 10s 95ms/step - loss: 0.4163 - accuracy: 0.8408\n",
      "Epoch 4/20\n",
      "103/103 [==============================] - 10s 98ms/step - loss: 0.2713 - accuracy: 0.9095\n",
      "Epoch 5/20\n",
      "103/103 [==============================] - 9s 91ms/step - loss: 0.1767 - accuracy: 0.9474\n",
      "Epoch 6/20\n",
      "103/103 [==============================] - 10s 95ms/step - loss: 0.2434 - accuracy: 0.9318\n",
      "Epoch 7/20\n",
      "103/103 [==============================] - 11s 107ms/step - loss: 0.1307 - accuracy: 0.9653\n",
      "Epoch 8/20\n",
      "103/103 [==============================] - 11s 106ms/step - loss: 0.1265 - accuracy: 0.9659\n",
      "Epoch 9/20\n",
      "103/103 [==============================] - 10s 99ms/step - loss: 0.0398 - accuracy: 0.9893\n",
      "Epoch 10/20\n",
      "103/103 [==============================] - 11s 103ms/step - loss: 0.0460 - accuracy: 0.9889\n",
      "Epoch 11/20\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.0325 - accuracy: 0.9910\n",
      "Epoch 12/20\n",
      "103/103 [==============================] - 10s 101ms/step - loss: 0.0354 - accuracy: 0.9907\n",
      "Epoch 13/20\n",
      "103/103 [==============================] - 11s 106ms/step - loss: 0.0275 - accuracy: 0.9918\n",
      "Epoch 14/20\n",
      "103/103 [==============================] - 9s 90ms/step - loss: 0.0186 - accuracy: 0.9942\n",
      "Epoch 15/20\n",
      "103/103 [==============================] - 10s 100ms/step - loss: 0.0169 - accuracy: 0.9939\n",
      "Epoch 16/20\n",
      "103/103 [==============================] - 9s 91ms/step - loss: 0.2380 - accuracy: 0.9229\n",
      "Epoch 17/20\n",
      "103/103 [==============================] - 9s 90ms/step - loss: 0.1292 - accuracy: 0.9612\n",
      "Epoch 18/20\n",
      "103/103 [==============================] - 9s 91ms/step - loss: 0.0311 - accuracy: 0.9906\n",
      "Epoch 19/20\n",
      "103/103 [==============================] - 9s 89ms/step - loss: 0.0273 - accuracy: 0.9910\n",
      "Epoch 20/20\n",
      "103/103 [==============================] - 10s 96ms/step - loss: 0.0219 - accuracy: 0.9922\n",
      "52/52 [==============================] - 2s 32ms/step - loss: 0.4647 - accuracy: 0.8879\n",
      "Mean accuracy from cross-validation: 0.8736321330070496\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entraînement du modèle avec cross-validation\n",
    "print(\"Entraînement du modèle avec cross-validation...\")\n",
    "accuracies = []\n",
    "for train_index, val_index in cv.split(X_train, np.argmax(y_train, axis=1)):\n",
    "    model = create_model()\n",
    "    model.fit(X_train[train_index], y_train[train_index], epochs=20, batch_size=64)\n",
    "    scores = model.evaluate(X_train[val_index], y_train[val_index])\n",
    "    accuracies.append(scores[1])\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean accuracy from cross-validation: {mean_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle avec X_train et y_train...\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 13s 85ms/step - loss: 1.3436 - accuracy: 0.4490\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 12s 95ms/step - loss: 0.8435 - accuracy: 0.6881\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 13s 99ms/step - loss: 1.0456 - accuracy: 0.6462\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 12s 96ms/step - loss: 0.8693 - accuracy: 0.6987\n",
      "Epoch 5/20\n",
      "129/129 [==============================] - 13s 104ms/step - loss: 0.7004 - accuracy: 0.7782\n",
      "Epoch 6/20\n",
      "129/129 [==============================] - 13s 104ms/step - loss: 0.4254 - accuracy: 0.8730\n",
      "Epoch 7/20\n",
      "129/129 [==============================] - 13s 104ms/step - loss: 0.3028 - accuracy: 0.9087\n",
      "Epoch 8/20\n",
      "129/129 [==============================] - 13s 104ms/step - loss: 0.2469 - accuracy: 0.9287\n",
      "Epoch 9/20\n",
      "129/129 [==============================] - 15s 114ms/step - loss: 0.1085 - accuracy: 0.9722\n",
      "Epoch 10/20\n",
      "129/129 [==============================] - 14s 106ms/step - loss: 0.0600 - accuracy: 0.9849\n",
      "Epoch 11/20\n",
      "129/129 [==============================] - 13s 101ms/step - loss: 0.0353 - accuracy: 0.9906\n",
      "Epoch 12/20\n",
      "129/129 [==============================] - 12s 91ms/step - loss: 0.0265 - accuracy: 0.9926\n",
      "Epoch 13/20\n",
      "129/129 [==============================] - 14s 105ms/step - loss: 0.0224 - accuracy: 0.9932\n",
      "Epoch 14/20\n",
      "129/129 [==============================] - 15s 113ms/step - loss: 0.1403 - accuracy: 0.9561\n",
      "Epoch 15/20\n",
      "129/129 [==============================] - 15s 118ms/step - loss: 0.1239 - accuracy: 0.9645\n",
      "Epoch 16/20\n",
      "129/129 [==============================] - 14s 108ms/step - loss: 0.1067 - accuracy: 0.9704\n",
      "Epoch 17/20\n",
      "129/129 [==============================] - 15s 119ms/step - loss: 0.0427 - accuracy: 0.9889\n",
      "Epoch 18/20\n",
      "129/129 [==============================] - 15s 115ms/step - loss: 0.0269 - accuracy: 0.9916\n",
      "Epoch 19/20\n",
      "129/129 [==============================] - 15s 116ms/step - loss: 0.0160 - accuracy: 0.9944\n",
      "Epoch 20/20\n",
      "129/129 [==============================] - 15s 120ms/step - loss: 0.0218 - accuracy: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e8adb969a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle avec X_train et y_train\n",
    "print(\"Entraînement du modèle avec X_train et y_train...\")\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions sur l'ensemble de test...\n"
     ]
    }
   ],
   "source": [
    "# Prédictions sur l'ensemble de test\n",
    "print(\"Prédictions sur l'ensemble de test...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8568646543330087\n",
      "Normalized confusion matrix:\n",
      "[[0.92241379 0.00287356 0.0316092  0.03735632 0.00574713]\n",
      " [0.00539084 0.86522911 0.06469003 0.01347709 0.05121294]\n",
      " [0.0776699  0.15048544 0.55825243 0.05825243 0.15533981]\n",
      " [0.07435897 0.03333333 0.06153846 0.78717949 0.04358974]\n",
      " [0.00405954 0.0202977  0.01894452 0.01488498 0.94181326]]\n"
     ]
    }
   ],
   "source": [
    "# Calcul de l'accuracy\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "# Calcul de la matrice de confusion normalisée\n",
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "print(\"Normalized confusion matrix:\")\n",
    "print(conf_matrix_normalized)\n",
    "\n",
    "model_directory = \"model\"\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde du modèle...\n",
      "Modèle sauvegardé.\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde du modèle\n",
    "print(\"Sauvegarde du modèle...\")\n",
    "model.save_weights(f\"{model_directory}/LSTM_model_weights.h5\")\n",
    "print(\"Modèle sauvegardé.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
